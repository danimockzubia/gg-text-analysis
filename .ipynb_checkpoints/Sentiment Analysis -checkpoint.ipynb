{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ec9edaf",
   "metadata": {},
   "source": [
    "The goal of this project was to combine two of my primary interests: teen dramas and American politics. In this project, I will investigate what impact, if any, 9/11 had on the television series Gilmore Girls."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8eedaa3",
   "metadata": {},
   "source": [
    "The data used for this project, that is the transcription of Gilmore Girls episodes was obtained from https://www.gilmoregirls.org. This included a handful of episodes from both seasons 1 and 3. Season two was omitted as the episodes aired only a short time after the events of 9/11 and were most likely written before 9/11/2001."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ef1aa9",
   "metadata": {},
   "source": [
    "Sentiment analysis is described as "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7de8796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56e408d",
   "metadata": {},
   "source": [
    "The above packages will help us plot the frequency of words later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2e4856",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0bcb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import gensim\n",
    "#import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75b28bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6489316f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = set(stopwords.words('english'))\n",
    "print(stops)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68546f28",
   "metadata": {},
   "source": [
    "nltk allows to tokenize the  text and remove certain stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76f62a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385bbff5",
   "metadata": {},
   "source": [
    "beautiful soup allows to easily scrape text from given url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913d0066",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf6571a",
   "metadata": {},
   "source": [
    "requests allows to get the url where desired text is located"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da58283e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import seaborn as sns\n",
    "#import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2736377",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordsFromPage(pageNumber):\n",
    "    url = 'https://www.gilmoregirls.org/eguide/transcripts/episode{pageNumber}.html'\n",
    "    url = url.format(pageNumber=pageNumber)\n",
    "\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    data = ''\n",
    "    for data in soup.find_all('body'):\n",
    "        script = data.get_text()    \n",
    "        get_words = word_tokenize(script)\n",
    "        lower_words = [x.lower() for x in get_words]\n",
    "        \n",
    "        new_words = ' '.join(ch for ch in lower_words if ch.isalnum())\n",
    "        \n",
    "        data = new_words\n",
    "        stopwords = nltk.corpus.stopwords.words('english')\n",
    "        GGStops = [\"oh\", \"rory\", \"lorelai\", \"ah\", \"ooh\", \"l\", \"r\", \"emily\", \"max\", \n",
    "                    \"luke\", \"lane\", \"richard\", \"know\", \"well\", \"really\", \"okay\", \"sookie\", \n",
    "                    \"would\", \"like\", \"dean\", \"go\", \"paris\", \"zach\", \"dave\", \"terry\", \"michel\", \"kyle\", \n",
    "                   \"brian\" \"louise\", \"jackson\", \"yes\", \"yeah\", \"kirk\", \"taylor\", \"darren\", 'darren'\n",
    "                   \"louise\", \"madeline\", \"tristan\", \"jess\", \"ok\", \"going\", \"okay\", \"get\", \"got\",\n",
    "                    \"think\", \"bye\", \"hi\", \"uh\", \"somthing\", \"gon\", \"na\", \"tell\", \"one\", 'grandma', \n",
    "                  'grandpa', 'grandmother', '1', 'chilton', 'something', 'carol', \"site\", 'navigation'\n",
    "                  'transcript', 'navigation', 'summary', 'cast', 'characters', 'episode', 'guide',\n",
    "                  'drella']\n",
    "        stopwords.extend(GGStops)\n",
    "        words = word_tokenize(data)\n",
    "        wordsFiltered = []\n",
    "\n",
    "        for w in words:\n",
    "            if w not in stopwords:\n",
    "                wordsFiltered.append(w)\n",
    "                    \n",
    "        return(wordsFiltered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbf8d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "season_1 = (getWordsFromPage(1)) + (getWordsFromPage(2)) + (getWordsFromPage(3)) + (getWordsFromPage(4))\n",
    "season_3 = (getWordsFromPage(301)) + (getWordsFromPage(302)) + (getWordsFromPage(303)) + (getWordsFromPage(304))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387e764d",
   "metadata": {},
   "source": [
    "here we concatenated the word lists from episodes within a given season and assigned them to a variable denoting their season "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2b847e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(season_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201d278d",
   "metadata": {},
   "source": [
    "here we can look at a list of words from each given episode. for season one episodes are simply 1 - 21.\n",
    "for further seasons, they are formated as such: season 2 episode 1 = 201 etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbf4062",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd5f82b",
   "metadata": {},
   "source": [
    "word cloud lets us make a word cloud from the list off words :) unbelievable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3343c438",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(background_color=\"white\", max_words=150, contour_width=1, contour_color='steelblue')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f8ad52",
   "metadata": {},
   "source": [
    "~make it pretty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a401b455",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud.generate(str(getWordsFromPage(319)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb75eefa",
   "metadata": {},
   "source": [
    "get a word cloud for a specific episode "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d987c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud.generate(str(season_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d588322",
   "metadata": {},
   "source": [
    "get a word cloud with pre-defined seasons variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb160d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud.to_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bde275",
   "metadata": {},
   "source": [
    "visualize the word cloud!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fa376b",
   "metadata": {},
   "source": [
    "another visualization, a graph representing the frequency of words in a given season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579eb590",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "nlp_words=nltk.FreqDist(season_1)\n",
    "nlp_words.plot(25);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a39dfc6",
   "metadata": {},
   "source": [
    "Turning the list of words from season 1 into a data frame to be able to perform sentiment analysis using VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edc8b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "data_set1 = season_1\n",
    "Counter = Counter(data_set1)\n",
    "\n",
    "most_occur = Counter.most_common(20)\n",
    "  \n",
    "print(most_occur)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24bdd04",
   "metadata": {},
   "source": [
    "the words that occur the most in season 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a26589",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "data_set2 = season_2\n",
    "Counter = Counter(data_set2)\n",
    "\n",
    "most_occur = Counter.most_common(100)\n",
    "  \n",
    "print(most_occur)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4091f610",
   "metadata": {},
   "source": [
    "the words that occur the most in season 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be223908",
   "metadata": {},
   "outputs": [],
   "source": [
    "li1 = season_3\n",
    "li2 = season_1\n",
    " \n",
    "wordsin1 = []\n",
    "for element in li1:\n",
    "    if element not in li2:\n",
    "        wordsin1.append(element)\n",
    " \n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "data_set = wordsin1\n",
    "Counter = Counter(wordsin1)\n",
    "\n",
    "most_occur = Counter.most_common(110)\n",
    "  \n",
    "print(most_occur)\n",
    "#print(temp3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4641960",
   "metadata": {},
   "source": [
    "words occurring in season 1 and not season 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a07e806",
   "metadata": {},
   "outputs": [],
   "source": [
    "li1 = season_1\n",
    "li2 = season_3\n",
    " \n",
    "wordsin3 = []\n",
    "for element in li1:\n",
    "    if element not in li2:\n",
    "        wordsin3.append(element)\n",
    " \n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "data_set = wordsin3\n",
    "Counter = Counter(wordsin3)\n",
    "\n",
    "most_occur = Counter.most_common(100)\n",
    "  \n",
    "print(most_occur)\n",
    "#print(temp3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c34f8d",
   "metadata": {},
   "source": [
    "words occurring in season 3 and not season 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
