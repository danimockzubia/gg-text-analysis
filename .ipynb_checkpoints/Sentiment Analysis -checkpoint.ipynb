{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbf680e8",
   "metadata": {},
   "source": [
    "INTRODUCTION\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d840b85e",
   "metadata": {},
   "source": [
    "The goal of this project was primarily to combine two of my primary interests: teen dramas and American politics. In this project, I investigated what impact, if any, 9/11 had on the television series Gilmore Girls through dictionary based sentiment analysis and frequency distribution. Gilmore Girls is a show that is known for the sheer amount of language in any given episodes. The titular characters, Rory and Lorelai Gilmore, are known for their fast-talking and immense repertoire of cultural references. As such, this made Gilmore Girls an interesting choice for an analysis on such a monumental global event. \n",
    "\n",
    "Through sentiment and frequency distribution analysis, I sought to uncover any change in words related to 9/11 between seasons one and three of Gilmore Girls, which were released in 2000 and 2002 respectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b11123",
   "metadata": {},
   "source": [
    "The data used for this project, that is the transcription of Gilmore Girls episodes was obtained from https://www.gilmoregirls.org. This included a handful of episodes from both seasons 1 and 3 scraped from the website cited above using BeautifulSoup. Season two was omitted as the episodes aired only a short time after the events of 9/11 and were most likely written before 9/11/2001. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d432906c",
   "metadata": {},
   "source": [
    "CODE\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10d80333",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cd890b",
   "metadata": {},
   "source": [
    "The above packages will help us plot the frequency of words later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91694932",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "#import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6c9b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import gensim\n",
    "#import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c493dd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33c86da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = set(stopwords.words('english'))\n",
    "#print(stops)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d4f4c6",
   "metadata": {},
   "source": [
    "Nltk allows us to tokenize the text, i.e., transforming a large block of text into single words, and remove certain stop words. The list of stop words provided by the nltk dictionary includes words like \"because\", \"only\", \"am\", which that are commonly used in spoken and written language. These words detract from other important words in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "913d0066",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c76f62a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6929ca",
   "metadata": {},
   "source": [
    "Importing 'requests' allows to get the HTML from a given url where desired text is located. This is how we will get the transcripts of the Gilmore Girls episodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fbf7fc",
   "metadata": {},
   "source": [
    "BeautifulSoup allows us to easily parse data that has been has been scraped from the web. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da58283e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import seaborn as sns\n",
    "#import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9be94ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordsFromPage(pageNumber):\n",
    "    url = 'https://www.gilmoregirls.org/eguide/transcripts/episode{pageNumber}.html'\n",
    "    url = url.format(pageNumber=pageNumber)\n",
    "\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    data = ''\n",
    "    for data in soup.find_all('body'):\n",
    "        script = data.get_text()    \n",
    "        get_words = word_tokenize(script)\n",
    "        lower_words = [x.lower() for x in get_words]\n",
    "        \n",
    "        new_words = ' '.join(ch for ch in lower_words if ch.isalnum())\n",
    "        \n",
    "        data = new_words\n",
    "        stopwords = nltk.corpus.stopwords.words('english')\n",
    "        GGStops = [\"morey\",\"oh\", \"rory\", \"lorelai\", \"ah\", \"ooh\", \"l\", \"r\", \"emily\", \"max\", \n",
    "                    \"luke\", \"lane\", \"richard\", \"know\", \"well\", \"really\", \"okay\", \"sookie\", \n",
    "                    \"would\", \"like\", \"dean\", \"go\", \"paris\", \"zach\", \"dave\", \"terry\", \"michel\", \"kyle\", \n",
    "                   \"brian\" \"louise\", \"jackson\", \"yes\", \"yeah\", \"kirk\", \"taylor\", \"darren\", 'darren'\n",
    "                   \"louise\", \"madeline\", \"tristan\", \"jess\", \"ok\", \"going\", \"okay\", \"get\", \"got\",\n",
    "                    \"think\", \"bye\", \"hi\", \"uh\", \"somthing\", \"gon\", \"na\", \"tell\", \"one\", 'grandma', \n",
    "                  'grandpa', 'grandmother', '1', 'chilton', 'something', 'carol', \"site\", 'navigation'\n",
    "                  'transcript', 'navigation', 'summary', 'cast', 'characters', 'episode', 'guide',\n",
    "                  'drella', \"said\", \"say\", \"joey\", 'debbie', 'jamie', 'jennifer', 'ian']\n",
    "        stopwords.extend(GGStops)\n",
    "        words = word_tokenize(data)\n",
    "        wordsFiltered = []\n",
    "\n",
    "        for w in words:\n",
    "            if w not in stopwords:\n",
    "                wordsFiltered.append(w)\n",
    "                    \n",
    "        return(wordsFiltered)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d861907",
   "metadata": {},
   "source": [
    "In the above code, we created a function that retrieves the script data from the url (https://www.gilmoregirls.org/eguide/transcripts/episode{pageNumber}.html) by the url page number which corresponds to the season and episode number. Using BeautifulSoup, we parse the retrieved text into single words. We then transform all of the words into their lowercase form. Using the 'isalnum()' method, we remove any characters that are not alphanumerical, like punctuation. \n",
    "\n",
    "Using nltk we were able to remove common English stopwords. However, upon inspection of the data, we find that character names, expressions like \"oh,\" and other unimportant words prevent us from focusing on the important words. Therefore, we appended a new list (GGStops) onto the list of stopwords to filter out the unwanted words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "262072ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "season_1 = (getWordsFromPage(1)) + (getWordsFromPage(2)) + (getWordsFromPage(3)) + (getWordsFromPage(4)) + (getWordsFromPage(5)) \n",
    "season_3 = (getWordsFromPage(301)) + (getWordsFromPage(302)) + (getWordsFromPage(303)) + (getWordsFromPage(304)) + (getWordsFromPage(305)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce15930",
   "metadata": {},
   "source": [
    "here we concatenated the word lists from episodes within a given season and assigned them to a variable denoting their season "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cf2e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(season_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa60137",
   "metadata": {},
   "source": [
    "Using the print function we can look at a list of words from each given episode. for season one episodes are simply 1 - 21.\n",
    "for further seasons, they are formated as such: season 2 episode 1 = 201 etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbf4062",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4773a3",
   "metadata": {},
   "source": [
    "WordCloud lets us make a word cloud from the list off words :) unbelievable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3343c438",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(background_color=\"white\", max_words=150, contour_width=1, contour_color='steelblue')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6191ed87",
   "metadata": {},
   "source": [
    "~make it pretty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a401b455",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud.generate(str(getWordsFromPage(319)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c9badc",
   "metadata": {},
   "source": [
    "get a word cloud for a specific episode "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4038d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud.generate(str(season_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a896da",
   "metadata": {},
   "source": [
    "get a word cloud with pre-defined seasons variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb160d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud.to_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1c638c",
   "metadata": {},
   "source": [
    "visualize the word cloud!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543095c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "nlp_words=nltk.FreqDist(season_1)\n",
    "nlp_words.plot(25);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3cdc5e",
   "metadata": {},
   "source": [
    "another visualization, a graph representing the frequency of words in a given season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca507a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "data_set1 = season_1\n",
    "Counter = Counter(data_set1)\n",
    "\n",
    "most_occur = Counter.most_common(50)\n",
    "  \n",
    "#print(most_occur)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87a1d40",
   "metadata": {},
   "source": [
    "the words that occur the most in season 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d5fc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "data_set2 = season_3\n",
    "Counter = Counter(data_set2)\n",
    "\n",
    "most_occur = Counter.most_common(50)\n",
    "  \n",
    "#print(most_occur)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77981933",
   "metadata": {},
   "source": [
    "the words that occur the most in season 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d72626",
   "metadata": {},
   "outputs": [],
   "source": [
    "li1 = season_1\n",
    "li2 = season_3\n",
    " \n",
    "wordsin1 = []\n",
    "for element in li1:\n",
    "    if element not in li2:\n",
    "        wordsin1.append(element)\n",
    " \n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "data_set = wordsin1\n",
    "Counter = Counter(wordsin1)\n",
    "\n",
    "most_occur = Counter.most_common(110)\n",
    "  \n",
    "#print(most_occur)\n",
    "#print(wordsin1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d291094",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "w1_words=nltk.FreqDist(wordsin1)\n",
    "w1_words.plot(25);\n",
    "\n",
    "w1_words.tabulate(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2907bd",
   "metadata": {},
   "source": [
    "words occurring in season 1 and not season 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e97a5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "li1 = season_1\n",
    "li2 = season_3\n",
    " \n",
    "wordsin3 = []\n",
    "for element in li2:\n",
    "    if element not in li1:\n",
    "        wordsin3.append(element)\n",
    " \n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "data_set = wordsin3\n",
    "Counter = Counter(wordsin3)\n",
    "\n",
    "most_occur = Counter.most_common(100)\n",
    "  \n",
    "#print(most_occur)\n",
    "#print(wordsin3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9bdd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "w3_words=nltk.FreqDist(wordsin3)\n",
    "w3_words.plot(25);\n",
    "\n",
    "w3_words.tabulate(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a909184",
   "metadata": {},
   "source": [
    "words occurring in season 3 and not season 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce84fb54",
   "metadata": {},
   "source": [
    "Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39d91a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
