{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d840b85e",
   "metadata": {},
   "source": [
    "The goal of this project was to combine two of my primary interests: teen dramas and American politics. In this project, I will investigate what impact, if any, 9/11 had on the television series Gilmore Girls."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b11123",
   "metadata": {},
   "source": [
    "The data used for this project, that is the transcription of Gilmore Girls episodes was obtained from https://www.gilmoregirls.org. This included a handful of episodes from both seasons 1 and 3. Season two was omitted as the episodes aired only a short time after the events of 9/11 and were most likely written before 9/11/2001."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d432906c",
   "metadata": {},
   "source": [
    "Sentiment analysis is described as "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d80333",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cd890b",
   "metadata": {},
   "source": [
    "The above packages will help us plot the frequency of words later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91694932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6c9b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import gensim\n",
    "#import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c493dd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('vader_lexicon')\n",
    "#from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "#sid = SentimentIntensityAnalyzer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c86da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = set(stopwords.words('english'))\n",
    "#print(stops)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d4f4c6",
   "metadata": {},
   "source": [
    "nltk allows to tokenize the  text and remove certain stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76f62a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fbf7fc",
   "metadata": {},
   "source": [
    "beautiful soup allows to easily scrape text from given url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913d0066",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6929ca",
   "metadata": {},
   "source": [
    "requests allows to get the url where desired text is located"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da58283e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import seaborn as sns\n",
    "#import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be94ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordsFromPage(pageNumber):\n",
    "    url = 'https://www.gilmoregirls.org/eguide/transcripts/episode{pageNumber}.html'\n",
    "    url = url.format(pageNumber=pageNumber)\n",
    "\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    data = ''\n",
    "    for data in soup.find_all('body'):\n",
    "        script = data.get_text()    \n",
    "        get_words = word_tokenize(script)\n",
    "        lower_words = [x.lower() for x in get_words]\n",
    "        \n",
    "        new_words = ' '.join(ch for ch in lower_words if ch.isalnum())\n",
    "        \n",
    "        data = new_words\n",
    "        stopwords = nltk.corpus.stopwords.words('english')\n",
    "        GGStops = [\"oh\", \"rory\", \"lorelai\", \"ah\", \"ooh\", \"l\", \"r\", \"emily\", \"max\", \n",
    "                    \"luke\", \"lane\", \"richard\", \"know\", \"well\", \"really\", \"okay\", \"sookie\", \n",
    "                    \"would\", \"like\", \"dean\", \"go\", \"paris\", \"zach\", \"dave\", \"terry\", \"michel\", \"kyle\", \n",
    "                   \"brian\" \"louise\", \"jackson\", \"yes\", \"yeah\", \"kirk\", \"taylor\", \"darren\", 'darren'\n",
    "                   \"louise\", \"madeline\", \"tristan\", \"jess\", \"ok\", \"going\", \"okay\", \"get\", \"got\",\n",
    "                    \"think\", \"bye\", \"hi\", \"uh\", \"somthing\", \"gon\", \"na\", \"tell\", \"one\", 'grandma', \n",
    "                  'grandpa', 'grandmother', '1', 'chilton', 'something', 'carol', \"site\", 'navigation'\n",
    "                  'transcript', 'navigation', 'summary', 'cast', 'characters', 'episode', 'guide',\n",
    "                  'drella', \"said\", \"say\", \"joey\", 'debbie', 'jamie', 'jennifer', 'ian']\n",
    "        stopwords.extend(GGStops)\n",
    "        words = word_tokenize(data)\n",
    "        wordsFiltered = []\n",
    "\n",
    "        for w in words:\n",
    "            if w not in stopwords:\n",
    "                wordsFiltered.append(w)\n",
    "                    \n",
    "        return(wordsFiltered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262072ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "season_1 = (getWordsFromPage(1)) + (getWordsFromPage(2)) + (getWordsFromPage(3)) + (getWordsFromPage(4)) + (getWordsFromPage(5)) \n",
    "season_3 = (getWordsFromPage(301)) + (getWordsFromPage(302)) + (getWordsFromPage(303)) + (getWordsFromPage(304)) + (getWordsFromPage(305)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce15930",
   "metadata": {},
   "source": [
    "here we concatenated the word lists from episodes within a given season and assigned them to a variable denoting their season "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cf2e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(season_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa60137",
   "metadata": {},
   "source": [
    "here we can look at a list of words from each given episode. for season one episodes are simply 1 - 21.\n",
    "for further seasons, they are formated as such: season 2 episode 1 = 201 etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbf4062",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4773a3",
   "metadata": {},
   "source": [
    "word cloud lets us make a word cloud from the list off words :) unbelievable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3343c438",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(background_color=\"white\", max_words=150, contour_width=1, contour_color='steelblue')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6191ed87",
   "metadata": {},
   "source": [
    "~make it pretty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a401b455",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud.generate(str(getWordsFromPage(319)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c9badc",
   "metadata": {},
   "source": [
    "get a word cloud for a specific episode "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4038d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud.generate(str(season_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a896da",
   "metadata": {},
   "source": [
    "get a word cloud with pre-defined seasons variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb160d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud.to_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1c638c",
   "metadata": {},
   "source": [
    "visualize the word cloud!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543095c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "nlp_words=nltk.FreqDist(season_1)\n",
    "nlp_words.plot(25);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3cdc5e",
   "metadata": {},
   "source": [
    "another visualization, a graph representing the frequency of words in a given season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca507a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "data_set1 = season_1\n",
    "Counter = Counter(data_set1)\n",
    "\n",
    "most_occur = Counter.most_common(50)\n",
    "  \n",
    "print(most_occur)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87a1d40",
   "metadata": {},
   "source": [
    "the words that occur the most in season 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d5fc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "data_set2 = season_3\n",
    "Counter = Counter(data_set2)\n",
    "\n",
    "most_occur = Counter.most_common(50)\n",
    "  \n",
    "print(most_occur)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77981933",
   "metadata": {},
   "source": [
    "the words that occur the most in season 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d72626",
   "metadata": {},
   "outputs": [],
   "source": [
    "li1 = season_3\n",
    "li2 = season_1\n",
    " \n",
    "wordsin1 = []\n",
    "for element in li1:\n",
    "    if element not in li2:\n",
    "        wordsin1.append(element)\n",
    " \n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "data_set = wordsin1\n",
    "Counter = Counter(wordsin1)\n",
    "\n",
    "most_occur = Counter.most_common(110)\n",
    "  \n",
    "#print(most_occur)\n",
    "print(wordsin1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d291094",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "nlp_words=nltk.FreqDist(wordsin1)\n",
    "nlp_words.plot(25);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2907bd",
   "metadata": {},
   "source": [
    "words occurring in season 1 and not season 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e97a5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "li1 = season_1\n",
    "li2 = season_3\n",
    " \n",
    "wordsin3 = []\n",
    "for element in li1:\n",
    "    if element not in li2:\n",
    "        wordsin3.append(element)\n",
    " \n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "data_set = wordsin3\n",
    "Counter = Counter(wordsin3)\n",
    "\n",
    "most_occur = Counter.most_common(100)\n",
    "  \n",
    "#print(most_occur)\n",
    "print(wordsin3)\n",
    "#print(temp3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a909184",
   "metadata": {},
   "source": [
    "words occurring in season 3 and not season 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9bdd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "nlp_words=nltk.FreqDist(wordsin3)\n",
    "nlp_words.plot(25);\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
